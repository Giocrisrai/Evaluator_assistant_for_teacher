# config.py - Archivo de configuraci√≥n
import os
from typing import Dict, List
from dotenv import load_dotenv

# Cargar variables de entorno
load_dotenv()

class Config:
    """Configuraci√≥n del sistema de evaluaci√≥n."""
    
    # Proveedores de IA Disponibles
    LLM_PROVIDERS = {
        "github": {
            "base_url": "https://models.inference.ai.azure.com",
            "models": ["gpt-4o-mini", "gpt-3.5-turbo"],
            "free_tier": "S√≠ - L√≠mites generosos"
        },
        "gemini": {
            "models": ["gemini-pro"],
            "free_tier": "S√≠ - 60 requests/minuto"
        },
        "ollama": {
            "base_url": "http://localhost:11434",
            "models": ["llama3:latest", "codellama:latest", "gpt-oss:latest"],
            "free_tier": "S√≠ - Sin l√≠mites (local)"
        }
    }
    
    # Variables de entorno
    GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")
    LLM_API_KEY = os.getenv("LLM_API_KEY") 
    LLM_PROVIDER = os.getenv("LLM_PROVIDER", "ollama")
    
    # Configuraciones por defecto
    DEFAULT_OUTPUT_DIR = "./evaluaciones"
    DEFAULT_TIMEOUT = 300  # 5 minutos
    
    @classmethod
    def validate_config(cls):
        """Valida que la configuraci√≥n est√© completa."""
        missing = []
        
        if not cls.GITHUB_TOKEN:
            missing.append("GITHUB_TOKEN")
        if not cls.LLM_API_KEY and cls.LLM_PROVIDER not in ["ollama"]:
            missing.append("LLM_API_KEY")
        
        # Validar que Ollama est√© funcionando si es el proveedor seleccionado
        if cls.LLM_PROVIDER == "ollama":
            import requests
            try:
                response = requests.get(f"{cls.LLM_PROVIDERS['ollama']['base_url']}/api/tags", timeout=5)
                if response.status_code == 200:
                    print("‚úÖ Ollama est√° funcionando correctamente")
                else:
                    print("‚ùå Ollama no est√° respondiendo correctamente")
                    missing.append("OLLAMA_SERVER")
            except Exception as e:
                print(f"‚ùå No se puede conectar a Ollama: {e}")
                print("üí° Aseg√∫rate de que Ollama est√© ejecut√°ndose con: ollama serve")
                missing.append("OLLAMA_SERVER")
            
        if missing:
            print("‚ùå Configuraci√≥n incompleta. Variables faltantes:")
            for var in missing:
                print(f"   - {var}")
            print("\nCrea un archivo .env con:")
            print("GITHUB_TOKEN=tu_token_aqui")
            print("LLM_API_KEY=tu_api_key_aqui")  
            print("LLM_PROVIDER=ollama")
            return False
        
        print("‚úÖ Configuraci√≥n v√°lida")
        return True
