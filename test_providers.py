#!/usr/bin/env python3
"""
Script para probar ambos proveedores de LLM
Verifica que GitHub Models y Ollama funcionen correctamente
"""

import os
import sys
from dotenv import load_dotenv

# Agregar src al path
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from config import Config

def test_github_models():
    """Prueba GitHub Models."""
    print("ğŸ§ª PROBANDO GITHUB MODELS...")
    
    try:
        import openai
        
        client = openai.OpenAI(
            base_url=Config.LLM_PROVIDERS["github"]["base_url"],
            api_key=Config.GITHUB_TOKEN
        )
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "Eres un evaluador de cÃ³digo."},
                {"role": "user", "content": "Responde solo: 'GitHub Models funcionando'"}
            ],
            max_tokens=50,
            temperature=0.1
        )
        
        result = response.choices[0].message.content.strip()
        print(f"âœ… GitHub Models: {result}")
        return True
        
    except Exception as e:
        print(f"âŒ GitHub Models Error: {e}")
        return False

def test_ollama():
    """Prueba Ollama."""
    print("ğŸ§ª PROBANDO OLLAMA...")
    
    try:
        import requests
        
        payload = {
            "model": "llama3:latest",
            "prompt": "Responde solo: 'Ollama funcionando'",
            "stream": False,
            "options": {
                "temperature": 0.1,
                "num_predict": 50
            }
        }
        
        response = requests.post(
            f"{Config.LLM_PROVIDERS['ollama']['base_url']}/api/generate",
            json=payload,
            timeout=60
        )
        
        if response.status_code == 200:
            result = response.json()["response"].strip()
            print(f"âœ… Ollama: {result}")
            return True
        else:
            print(f"âŒ Ollama Error: {response.status_code} - {response.text}")
            return False
            
    except Exception as e:
        print(f"âŒ Ollama Error: {e}")
        return False

def main():
    """FunciÃ³n principal."""
    print("ğŸ”§ PROBADOR DE PROVEEDORES DE LLM")
    print("=" * 50)
    
    # Cargar configuraciÃ³n
    load_dotenv()
    
    print(f"ğŸ“‹ Proveedor actual: {Config.LLM_PROVIDER}")
    print(f"ğŸ”‘ GitHub Token: {'âœ… Configurado' if Config.GITHUB_TOKEN else 'âŒ Faltante'}")
    print(f"ğŸ”‘ LLM API Key: {'âœ… Configurado' if Config.LLM_API_KEY else 'âŒ Faltante'}")
    
    print("\n" + "="*50)
    
    # Probar ambos proveedores
    github_ok = test_github_models()
    print()
    ollama_ok = test_ollama()
    
    print("\n" + "="*50)
    print("ğŸ“Š RESULTADOS:")
    print(f"GitHub Models: {'âœ… Funcionando' if github_ok else 'âŒ Error'}")
    print(f"Ollama: {'âœ… Funcionando' if ollama_ok else 'âŒ Error'}")
    
    if github_ok and ollama_ok:
        print("\nğŸ‰ Â¡Ambos proveedores funcionan correctamente!")
        print("ğŸ’¡ Puedes cambiar entre ellos modificando LLM_PROVIDER en .env")
    elif github_ok:
        print("\nâš ï¸ Solo GitHub Models funciona")
        print("ğŸ’¡ AsegÃºrate de que Ollama estÃ© ejecutÃ¡ndose")
    elif ollama_ok:
        print("\nâš ï¸ Solo Ollama funciona")
        print("ğŸ’¡ Revisa tu token de GitHub")
    else:
        print("\nâŒ NingÃºn proveedor funciona")
        print("ğŸ’¡ Revisa tu configuraciÃ³n")

if __name__ == "__main__":
    main()
