#!/usr/bin/env python3
"""
Script de validaci√≥n para verificar que el sistema est√© configurado correctamente.
"""

import os
import sys
import subprocess
from pathlib import Path
from dotenv import load_dotenv

def print_header(title):
    """Imprime header formateado."""
    print(f"\n{'='*60}")
    print(f"üîç {title}")
    print(f"{'='*60}")

def check_environment():
    """Verifica el entorno Python."""
    print_header("VERIFICACI√ìN DEL ENTORNO")
    
    # Versi√≥n de Python
    version = sys.version_info
    if version >= (3, 8):
        print(f"‚úÖ Python {version.major}.{version.minor}.{version.micro} - Compatible")
    else:
        print(f"‚ùå Python {version.major}.{version.minor}.{version.micro} - Se requiere 3.8+")
        return False
    
    # Entorno virtual
    if hasattr(sys, 'real_prefix') or (hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix):
        print("‚úÖ Entorno virtual - Activo")
    else:
        print("‚ö†Ô∏è  Entorno virtual - No detectado (recomendado)")
    
    return True

def check_dependencies():
    """Verifica dependencias instaladas."""
    print_header("VERIFICACI√ìN DE DEPENDENCIAS")
    
    required_packages = [
        ("requests", "Comunicaci√≥n HTTP"),
        ("PyGithub", "API de GitHub"),
        ("openai", "API OpenAI/GitHub Models"),
        ("google-generativeai", "Google Gemini"),
        ("pandas", "Manejo de datos"),
        ("python-dotenv", "Variables de entorno")
    ]
    
    all_ok = True
    
    for package, description in required_packages:
        try:
            if package == 'PyGithub':
                import github
            elif package == 'google-generativeai':
                import google.generativeai
            elif package == 'python-dotenv':
                import dotenv
            else:
                __import__(package.lower().replace('-', '_'))
            print(f"‚úÖ {package} - Instalado ({description})")
        except ImportError:
            print(f"‚ùå {package} - No encontrado ({description})")
            all_ok = False
    
    return all_ok

def check_files():
    """Verifica que los archivos necesarios existan."""
    print_header("VERIFICACI√ìN DE ARCHIVOS")
    
    required_files = [
        ("src/rubrica_evaluator.py", "M√≥dulo principal"),
        ("src/config.py", "Configuraci√≥n"),
        ("simple_evaluator.py", "Interface simplificada"),
        ("requirements.txt", "Lista de dependencias"),
        (".env.example", "Plantilla de configuraci√≥n")
    ]
    
    optional_files = [
        (".env", "Configuraci√≥n de APIs"),
        ("evaluaciones/", "Directorio de resultados")
    ]
    
    all_required_ok = True
    
    for file_path, description in required_files:
        path = Path(file_path)
        if path.exists():
            print(f"‚úÖ {file_path} - Encontrado ({description})")
        else:
            print(f"‚ùå {file_path} - Faltante ({description})")
            all_required_ok = False
    
    print("\nArchivos opcionales:")
    for file_path, description in optional_files:
        path = Path(file_path)
        if path.exists():
            print(f"‚úÖ {file_path} - Encontrado ({description})")
        else:
            print(f"‚ö†Ô∏è  {file_path} - No encontrado ({description})")
    
    return all_required_ok

def check_configuration():
    """Verifica la configuraci√≥n de APIs."""
    print_header("VERIFICACI√ìN DE CONFIGURACI√ìN")
    
    # Cargar variables de entorno
    load_dotenv()
    
    # Verificar configuraci√≥n
    sys.path.append("src")
    try:
        from config import Config
        
        github_token = Config.GITHUB_TOKEN
        llm_api_key = Config.LLM_API_KEY
        llm_provider = Config.LLM_PROVIDER
        
        print(f"ü§ñ Proveedor LLM: {llm_provider}")
        
        if github_token:
            print(f"‚úÖ GitHub Token - Configurado ({len(github_token[:10])}...)")
        else:
            print("‚ùå GitHub Token - No configurado")
            return False
        
        if llm_provider == "ollama":
            print("‚úÖ LLM API Key - No requerido para Ollama")
        else:
            if llm_api_key:
                print(f"‚úÖ LLM API Key - Configurado ({len(llm_api_key[:10])}...)")
            else:
                print("‚ùå LLM API Key - No configurado")
                return False
        
        return True
        
    except ImportError as e:
        print(f"‚ùå Error importando configuraci√≥n: {e}")
        return False

def test_github_connection():
    """Prueba la conexi√≥n a GitHub."""
    print_header("PRUEBA DE CONEXI√ìN A GITHUB")
    
    try:
        load_dotenv()
        github_token = os.getenv("GITHUB_TOKEN")
        
        if not github_token:
            print("‚ùå Token de GitHub no configurado")
            return False
        
        from github import Github, Auth
        
        g = Github(auth=Auth.Token(github_token))
        user = g.get_user()
        
        print(f"‚úÖ Conectado como: {user.login}")
        rate_limit = g.get_rate_limit()
        print(f"üìä L√≠mites de API: {rate_limit.rate.remaining}/{rate_limit.rate.limit}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error conectando a GitHub: {e}")
        return False

def test_llm_connection():
    """Prueba la conexi√≥n al LLM."""
    print_header("PRUEBA DE CONEXI√ìN A LLM")
    
    try:
        load_dotenv()
        
        sys.path.append("src")
        from config import Config
        from rubrica_evaluator import LLMEvaluator
        
        if Config.LLM_PROVIDER == "ollama":
            print("üîß Probando conexi√≥n a Ollama...")
            try:
                import ollama
                models = ollama.list()
                print(f"‚úÖ Ollama conectado - {len(models['models'])} modelos disponibles")
                return True
            except Exception as e:
                print(f"‚ùå Error conectando a Ollama: {e}")
                return False
        
        else:
            print(f"üîß Probando conexi√≥n a {Config.LLM_PROVIDER}...")
            
            # Test b√°sico del evaluador
            evaluator = LLMEvaluator(Config.LLM_PROVIDER, Config.LLM_API_KEY)
            print(f"‚úÖ Cliente {Config.LLM_PROVIDER} inicializado correctamente")
            
            return True
            
    except Exception as e:
        print(f"‚ùå Error probando LLM: {e}")
        return False

def test_basic_functionality():
    """Prueba funcionalidad b√°sica del sistema."""
    print_header("PRUEBA DE FUNCIONALIDAD B√ÅSICA")
    
    try:
        sys.path.append("src")
        from rubrica_evaluator import create_kedro_rubrica, RubricaEvaluator
        
        # Probar creaci√≥n de r√∫brica
        rubrica_dict = create_kedro_rubrica()
        print(f"‚úÖ R√∫brica Kedro creada - {len(rubrica_dict['criterios'])} criterios")
        
        # Verificar ponderaciones
        total_ponderacion = sum(c["ponderacion"] for c in rubrica_dict["criterios"])
        if abs(total_ponderacion - 1.0) < 0.01:
            print(f"‚úÖ Ponderaciones correctas - Total: {total_ponderacion:.2f}")
        else:
            print(f"‚ö†Ô∏è  Ponderaciones incorrectas - Total: {total_ponderacion:.2f}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error en funcionalidad b√°sica: {e}")
        return False

def run_sample_evaluation():
    """Ejecuta una evaluaci√≥n de muestra."""
    print_header("EVALUACI√ìN DE MUESTRA")
    
    try:
        load_dotenv()
        sys.path.append("src")
        
        from config import Config
        
        if not Config.validate_config():
            print("‚ùå Configuraci√≥n incompleta - saltando evaluaci√≥n de muestra")
            return False
        
        print("üöÄ Ejecutando evaluaci√≥n de muestra...")
        print("üìç Repositorio: https://github.com/octocat/Hello-World")
        
        # Importar despu√©s de validar configuraci√≥n
        from rubrica_evaluator import RubricaEvaluator, create_kedro_rubrica
        
        evaluador = RubricaEvaluator(
            github_token=Config.GITHUB_TOKEN,
            llm_provider=Config.LLM_PROVIDER,
            llm_api_key=Config.LLM_API_KEY
        )
        
        rubrica_dict = create_kedro_rubrica()
        rubrica = evaluador.load_rubrica_from_dict(rubrica_dict)
        
        # Evaluaci√≥n de muestra con repo simple
        repo_url = "https://github.com/octocat/Hello-World"
        evaluacion = evaluador.evaluate_repository(repo_url, rubrica)
        
        print(f"‚úÖ Evaluaci√≥n completada")
        print(f"üìä Nota obtenida: {evaluacion.nota_final}/7.0")
        print(f"‚è±Ô∏è  Tiempo: {evaluacion.tiempo_evaluacion:.1f}s")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error en evaluaci√≥n de muestra: {e}")
        print("üí° Esto es normal si las APIs tienen l√≠mites o el repo no es accesible")
        return False

def generate_report(results):
    """Genera reporte de validaci√≥n."""
    print_header("REPORTE DE VALIDACI√ìN")
    
    total_checks = len(results)
    passed_checks = sum(1 for result in results.values() if result)
    
    print(f"üìä Resultados: {passed_checks}/{total_checks} verificaciones exitosas")
    
    if passed_checks == total_checks:
        print("üéâ ¬°Todas las verificaciones pasaron!")
        print("‚úÖ El sistema est√° listo para usar")
        status = "COMPLETO"
    elif passed_checks >= total_checks - 2:
        print("‚ö†Ô∏è  Configuraci√≥n casi completa")
        print("üîß Revisar elementos marcados como ‚ùå")
        status = "PARCIAL"
    else:
        print("‚ùå Configuraci√≥n incompleta")
        print("üîß Revisar configuraci√≥n antes de usar")
        status = "INCOMPLETO"
    
    print(f"\nüè∑Ô∏è  Estado: {status}")
    
    return status

def main():
    """Funci√≥n principal de validaci√≥n."""
    print("üîç VALIDADOR DE CONFIGURACI√ìN")
    print("Verificando que el sistema est√© configurado correctamente...")
    
    # Ejecutar verificaciones
    results = {
        "environment": check_environment(),
        "dependencies": check_dependencies(), 
        "files": check_files(),
        "configuration": check_configuration(),
        "github": test_github_connection(),
        "llm": test_llm_connection(),
        "functionality": test_basic_functionality()
    }
    
    # Evaluaci√≥n opcional (puede fallar sin afectar el resultado principal)
    print("\nüß™ PRUEBA OPCIONAL:")
    sample_result = run_sample_evaluation()
    if sample_result:
        results["sample_evaluation"] = True
    
    # Generar reporte final
    status = generate_report(results)
    
    # Mostrar pr√≥ximos pasos
    if status == "COMPLETO":
        print(f"\nüöÄ PR√ìXIMOS PASOS:")
        print("python simple_evaluator.py --repo https://github.com/tu-usuario/tu-proyecto")
    elif status == "PARCIAL":
        print(f"\nüîß ACCIONES RECOMENDADAS:")
        if not results.get("configuration"):
            print("‚Ä¢ Configurar tokens en archivo .env")
        if not results.get("github"):
            print("‚Ä¢ Verificar token de GitHub")
        if not results.get("llm"):
            print("‚Ä¢ Verificar API key del proveedor LLM")
    else:
        print(f"\nüìö AYUDA:")
        print("‚Ä¢ Ejecuta: python setup.py")
        print("‚Ä¢ Lee: README.md")
        print("‚Ä¢ Revisa: .env.example")
    
    return 0 if status == "COMPLETO" else 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
